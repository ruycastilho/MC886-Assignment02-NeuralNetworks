{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment #2\n",
    "\n",
    "Pedro Stramantinoli P. Cagliume Gomes 175955\n",
    "\n",
    "Ruy Castilho Barrichelo 177012\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_fashionMNIST_train():\n",
    "    csv_path = os.path.join(\"Data/fashion-mnist-dataset\", \"fashion-mnist_train.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_fashionMNIST_tests():\n",
    "    csv_path = os.path.join(\"Data/fashion-mnist-dataset\", \"fashion-mnist_test.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scalling e Normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize(train, test):\n",
    "    train_mean = np.mean(train, axis=0)\n",
    "    train = train - train_mean\n",
    "    test = test - train_mean\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "def scale(X):\n",
    "    max_array = np.max(X, axis=0)\n",
    "    X = X / max_array[None, :]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração de matrizes de Features e Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup inicial dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#carrega os dados\n",
    "data_train = load_fashionMNIST_train()\n",
    "data_test = load_fashionMNIST_tests()\n",
    "\n",
    "# Separação em Features e Target\n",
    "\n",
    "data_train_target = np.array(data_train[\"label\"])\n",
    "data_test_target = np.array(data_test[\"label\"])\n",
    "\n",
    "data_train = np.array(data_train)\n",
    "data_test = np.array(data_test)\n",
    "\n",
    "data_train = np.delete(data_train, 0, axis=1)\n",
    "data_test = np.delete(data_test, 0, axis=1)\n",
    "\n",
    "# Normalização\n",
    "normalized_data_train, normalized_data_test = normalize(data_train,data_test)\n",
    "\n",
    "# Scaling\n",
    "scaled_data_train = scale(normalized_data_train)\n",
    "scaled_data_test = scale(normalized_data_test)\n",
    "\n",
    "def getTrainSet():\n",
    "    return scaled_data_train, data_train_target\n",
    "\n",
    "def getTestSet():\n",
    "    return scaled_data_test,data_test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerador de conjuntos de Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Generation \n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# Retorna um gerador de indices\n",
    "def generate_sets(TRAINING_DATA,type='kfold'):\n",
    "\n",
    "    # Cross validation using train_test_split\n",
    "    if (type == 'split'):\n",
    "       return train_test_split(TRAINING_DATA,test_size=0.2,random_state=0)\n",
    "\n",
    "    # Cross validation using K-Fold\n",
    "    # K = 5, Shuffle = true, Seed = 21\n",
    "    elif (type == 'kfold'):\n",
    "        kfold_seed = 21\n",
    "\n",
    "        kfold = KFold(n_splits=5, shuffle=True, random_state=kfold_seed)\n",
    "        return kfold.split(TRAINING_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Units per Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer=784 \n",
    "units_per_hidden_layer=1024 # 2 to the tenth, since there are 10 classes\n",
    "output_layer=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions=20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes weights randomly\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "def weights1():\n",
    "    t1 = np.random.rand(input_layer, units_per_hidden_layer)\n",
    "    \n",
    "\n",
    "def weights2():\n",
    "    t2 = np.random.rand(output_layer, units_per_hidden_layer)\n",
    "    \n",
    "     \n",
    "# Adds biases, initialized with zeros\n",
    "\n",
    "def bias1():\n",
    "    b1 = np.zeros((1, units_per_hidden_layer))\n",
    "    return b1\n",
    "\n",
    "def bias2():\n",
    "    b2 = np.zeros((1, output_layer))\n",
    "    return b2\n",
    "\n",
    "def setupNN():\n",
    "    return weights1(), bias1(), weights2(), bias2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid and Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1+np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    return a(1 - a)\n",
    "\n",
    "def loss(h, y):\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ReLU(z):\n",
    "    z[z<0]=0\n",
    "    return z\n",
    "\n",
    "def ReLU_derivative(z):\n",
    "    z = np.where(z >= 0, 1, 0)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def leaky_ReLU(z):\n",
    "    z = np.where(z > 0, z, z * 0.01)\n",
    "    return z\n",
    "\n",
    "def leaky_ReLU_derivative(z):\n",
    "    z = np.where(z >= 0, 1, 0.01)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Stable softmax\n",
    "def softmax(z):\n",
    "    expZ = np.exp(z - np.max(z))\n",
    "    return expZ / expZ.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigma(theta, pastSigma, derivative, z):\n",
    "    thetaTransp = np.transpose(theta)\n",
    "    mult = np.dot(thetaTransp, pastSigma)\n",
    "    deriv = derivative(z)\n",
    "    elementWise = np.multiply(mult, deriv)\n",
    "    return elementWise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function - Cross Entropy with Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(delta, y, length):\n",
    "    delta[range(lenght), y] -= 1\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success/Error Percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def error(prediction, y):\n",
    "    return np.mean( prediction != y )\n",
    "\n",
    "def success(prediction, y):\n",
    "    return np.mean( prediction == y )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "#### Different activation functions for hidden layers, but all of them end with the use of Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid_model(X, y):\n",
    "\n",
    "    W1, B1, W2, B2 = setupNN()\n",
    "\n",
    "    parameters = {}\n",
    "\n",
    "    for i in range(0, interactions):\n",
    "\n",
    "        # Forward propagation\n",
    "\n",
    "        z1 = np.dot(W1, X) + b1\n",
    "\n",
    "        a1 = sigmoid(z1)\n",
    "\n",
    "        z2 = np.dot(W2, a1) + b2\n",
    "\n",
    "        # Hypothesis\n",
    "        a2 = softmax(z2)\n",
    "\n",
    "        # Back propagation\n",
    "        delta3 = cross_entropy_loss(a2, y, len(X))\n",
    "\n",
    "        dW2 = np.dot(np.transpose(a1), delta3)\n",
    "        dB2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "\n",
    "        delta2 = np.multiply(np.dot(W2,delta3), sigmoid_derivative(a1))\n",
    "\n",
    "        dW1 = np.dot(np.transpose(X), delta2)\n",
    "        dB1 = np.sum(delta2, axis=0, keepdims=True)\n",
    "\n",
    "        # Updating parameters\n",
    "\n",
    "        W1 += -learning_rate * dW1\n",
    "        b1 += -learning_rate * db1\n",
    "        W2 += -learning_rate * dW2\n",
    "        b2 += -learning_rate * db2\n",
    "\n",
    "        parameters = {'W1': W1, 'B1': B1, 'W2': W2, 'B2': B2}\n",
    "\n",
    "def sigmoid_prediction(parameters, X):\n",
    "    W1, B1, W2, B2 = model['W1'], model['B1'], model['W2'], model['B2']\n",
    "\n",
    "    z1 = np.dot(W1, X) + b1\n",
    "    a1 = np.sigmoid(z1)\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = np.exp(z2)\n",
    "    probabilities = a2 / np.sum(a2, axis=1, keepdims=True)\n",
    "    \n",
    "    return np.argmax(probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-9d38053fe68b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0my1_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mmodel_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-b9209e827680>\u001b[0m in \u001b[0;36msigmoid_model\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Forward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "X1, y1 = getTrainSet()\n",
    "indices_generator = generate_sets(X1)\n",
    "model_list = []\n",
    "result_list = []\n",
    "\n",
    "i=0\n",
    "for train_index, val_index in indices_generator:\n",
    "    x1_train = X1[train_index]\n",
    "    y1_train = y1[train_index]\n",
    "    x1_val = X1[val_index]\n",
    "    y1_val = y1[val_index]\n",
    "     \n",
    "    parameters = sigmoid_model(x1_train, y1_train)\n",
    "    model_list.append(parameters)\n",
    "    \n",
    "    y1_predict = sigmoid_prediction(parameters, x1_val)\n",
    "    success = success(y1_predict, y1_val)\n",
    "    result_list.append(success)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "for i in range(0, len(result_list)):\n",
    "    print('Success rate for set #', i, ' is: ', result_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-52-8ec9cc791742>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-52-8ec9cc791742>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    a2 =\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a1 = X\n",
    "\n",
    "# Forward propagation\n",
    "\n",
    "z2 = np.dot(theta1, a1)\n",
    "\n",
    "a2 = \n",
    "\n",
    "z3 = np.dot(theta2, a2)\n",
    "\n",
    "# Hypothesis\n",
    "a3 = \n",
    "\n",
    "sigmaL = cost()\n",
    "\n",
    "# Back propagation\n",
    "\n",
    "sigma2 = sigma()\n",
    "\n",
    "\n",
    "delta = delta +\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-52-8ec9cc791742>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-52-8ec9cc791742>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    a2 =\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a1 = X\n",
    "\n",
    "# Forward propagation\n",
    "\n",
    "z2 = np.dot(theta1, a1)\n",
    "\n",
    "a2 = \n",
    "\n",
    "z3 = np.dot(theta2, a2)\n",
    "\n",
    "# Hypothesis\n",
    "a3 = \n",
    "\n",
    "sigmaL = cost()\n",
    "\n",
    "# Back propagation\n",
    "\n",
    "sigma2 = sigma()\n",
    "\n",
    "\n",
    "delta = delta +\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-52-8ec9cc791742>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-52-8ec9cc791742>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    a2 =\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a1 = X\n",
    "\n",
    "# Forward propagation\n",
    "\n",
    "z2 = np.dot(theta1, a1)\n",
    "\n",
    "a2 = \n",
    "\n",
    "z3 = np.dot(theta2, a2)\n",
    "\n",
    "# Hypothesis\n",
    "a3 = \n",
    "\n",
    "sigmaL = cost()\n",
    "\n",
    "# Back propagation\n",
    "\n",
    "sigma2 = sigma()\n",
    "\n",
    "\n",
    "delta = delta +\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
