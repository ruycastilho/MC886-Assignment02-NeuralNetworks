{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment #2\n",
    "\n",
    "Pedro Stramantinoli P. Cagliume Gomes 175955\n",
    "\n",
    "Ruy Castilho Barrichelo 177012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_fashionMNIST_train():\n",
    "    csv_path = os.path.join(\"Data/fashion-mnist-dataset\", \"fashion-mnist_train.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_fashionMNIST_tests():\n",
    "    csv_path = os.path.join(\"Data/fashion-mnist-dataset\", \"fashion-mnist_test.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scalling e Normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize(train, test):\n",
    "    train_mean = np.mean(train, axis=0)\n",
    "    train = train - train_mean\n",
    "    test = test - train_mean\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "def scale(X):\n",
    "    max_array = np.max(X, axis=0)\n",
    "    X = X / max_array[None, :]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração de matrizes de Features e Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup inicial dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#carrega os dados\n",
    "data_train = load_fashionMNIST_train()\n",
    "data_test = load_fashionMNIST_tests()\n",
    "\n",
    "# Separação em Features e Target\n",
    "\n",
    "data_train_target = np.array(data_train[\"label\"])\n",
    "data_test_target = np.array(data_test[\"label\"])\n",
    "\n",
    "data_train = np.array(data_train)\n",
    "data_test = np.array(data_test)\n",
    "\n",
    "data_train = np.delete(data_train, 0, axis=1)\n",
    "data_test = np.delete(data_test, 0, axis=1)\n",
    "\n",
    "# Normalização\n",
    "normalized_data_train, normalized_data_test = normalize(data_train,data_test)\n",
    "\n",
    "# Scaling\n",
    "scaled_data_train = scale(normalized_data_train)\n",
    "scaled_data_test = scale(normalized_data_test)\n",
    "\n",
    "# Adicionando coluna de 1's\n",
    "ready_data_train = np.c_[np.ones((len(scaled_data_train),1)), scaled_data_train]\n",
    "ready_data_test = np.c_[np.ones((len(scaled_data_test),1)), scaled_data_test]\n",
    "\n",
    "def getTrainSet():\n",
    "    return ready_data_train, data_train_target\n",
    "\n",
    "def getTestSet():\n",
    "    return ready_data_test,data_test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerador de conjuntos de Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Generation \n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# Retorna um gerador de indices\n",
    "def generate_sets(TRAINING_DATA,type='kfold'):\n",
    "\n",
    "    # Cross validation using train_test_split\n",
    "    if (type == 'split'):\n",
    "       return train_test_split(TRAINING_DATA,test_size=0.2,random_state=0)\n",
    "\n",
    "    # Cross validation using K-Fold\n",
    "    # K = 5, Shuffle = true, Seed = 21\n",
    "    elif (type == 'kfold'):\n",
    "        kfold_seed = 21\n",
    "\n",
    "        kfold = KFold(n_splits=5, shuffle=True, random_state=kfold_seed)\n",
    "        return kfold.split(TRAINING_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid and Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1+np.exp(-z))\n",
    "\n",
    "def loss(h, y):\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(z):\n",
    "    expZ = np.exp(z - np.max(z))\n",
    "    return expZ / np.sum(expZ, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success/Error Percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def error_rate(prediction, y):\n",
    "    return np.mean( prediction != y )\n",
    "\n",
    "def success_rate(prediction, y):\n",
    "    return np.mean( prediction == y )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def binary_logistic_regression(x, y, learningRate, numIterations):\n",
    "    xTrans = x.transpose()\n",
    "    theta = np.zeros((x.shape[1],1))\n",
    "    y = y.reshape((y.shape[0],1))\n",
    "    m = x.shape[0]\n",
    "\n",
    "    for i in range(numIterations):\n",
    "        z = np.dot(x, theta)\n",
    "        hypothesis = sigmoid(z)\n",
    "        \n",
    "        gradient = np.dot(xTrans, (hypothesis - y)) / m\n",
    "        theta -= learningRate * gradient\n",
    "        \n",
    "    return theta.reshape(x.shape[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the target according to the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setTarget(target,label):\n",
    "    return (target == label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Logistic Regression for each class (Binary Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "successForEachTheta = np.ndarray(5)\n",
    "errorForEachTheta = np.ndarray(5)\n",
    "\n",
    "X_Train,Target_Train = getTrainSet()\n",
    "\n",
    "targetForEachClass = transformTarget(Target_Train)\n",
    "thetaForEachClass = np.ndarray((5, X_Train.shape[1], 10))\n",
    "\n",
    "indices_generator = generate_sets(X_Train)\n",
    "\n",
    "layerIndex = 0\n",
    "for train_index, val_index in indices_generator:\n",
    "    # data for training and validation\n",
    "    x_train = X_Train[train_index]\n",
    "    y_train = Target_Train[train_index]\n",
    "    x_val = X_Train[val_index]\n",
    "    y_val = Target_Train[val_index]\n",
    "    \n",
    "    # calculate thetas for each class\n",
    "    for i in range(10):\n",
    "        transformedTarget = setTarget(y_train,i)\n",
    "        thetaForEachClass[layerIndex,:,i] = binary_logistic_regression(x_train, transformedTarget, 0.01, 500)\n",
    "    \n",
    "    # calculate the sigmoid\n",
    "    z = x_val.dot(thetaForEachClass[layerIndex,:,:])\n",
    "    h = sigmoid(z)\n",
    "    \n",
    "    # make de prediction and calculate both success and error for each Theta\n",
    "    y_predict = np.argmax(h,axis=1)\n",
    "    errorForEachTheta[layerIndex] = error_rate(y_predict,y_val)\n",
    "    successForEachTheta[layerIndex] = success_rate(y_predict,y_val)\n",
    "    layerIndex += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "successSoftmax = np.zeros(5)\n",
    "errorSoftmax = np.zeros(5)\n",
    "indices_generator = generate_sets(X_Train)\n",
    "layerIndex = 0\n",
    "\n",
    "for train_index, val_index in indices_generator:\n",
    "    # data for training and validation\n",
    "    x_train = X_Train[train_index]\n",
    "    y_train = Target_Train[train_index]\n",
    "    x_val = X_Train[val_index]\n",
    "    y_val = Target_Train[val_index]\n",
    "    \n",
    "    # calculate the softmax\n",
    "    z = x_val.dot(thetaForEachClass[layerIndex,:,:])\n",
    "    hypothesis = softmax(z)\n",
    "    \n",
    "    # make de prediction and calculate both success and error for each Theta\n",
    "    softMax_predict = np.argmax(hypothesis,axis=1)\n",
    "    errorSoftmax[layerIndex] = error_rate(softMax_predict,y_val)\n",
    "    successSoftmax[layerIndex] = success_rate(softMax_predict,y_val)\n",
    "    layerIndex += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta 0\n",
      "success: 0.7618333333333334\n",
      "softmax: 0.7618333333333334\n",
      "-------\n",
      "theta 1\n",
      "success: 0.7628333333333334\n",
      "softmax: 0.7628333333333334\n",
      "-------\n",
      "theta 2\n",
      "success: 0.76275\n",
      "softmax: 0.76275\n",
      "-------\n",
      "theta 3\n",
      "success: 0.7595\n",
      "softmax: 0.7595\n",
      "-------\n",
      "theta 4\n",
      "success: 0.7678333333333334\n",
      "softmax: 0.7678333333333334\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('theta ' + str(i))\n",
    "    print('success: ' + str(successForEachTheta[i]))\n",
    "    print('softmax: ' + str(successSoftmax[i]))\n",
    "    print('-------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
