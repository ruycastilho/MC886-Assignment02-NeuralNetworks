{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment #2\n",
    "\n",
    "Pedro Stramantinoli P. Cagliume Gomes 175955\n",
    "\n",
    "Ruy Castilho Barrichelo 177012\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_fashionMNIST_train():\n",
    "    csv_path = os.path.join(\"Data/fashion-mnist-dataset\", \"fashion-mnist_train.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_fashionMNIST_tests():\n",
    "    csv_path = os.path.join(\"Data/fashion-mnist-dataset\", \"fashion-mnist_test.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scalling e Normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize(train, test):\n",
    "    train_mean = np.mean(train, axis=0)\n",
    "    train = train - train_mean\n",
    "    test = test - train_mean\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "def scale(X):\n",
    "    max_array = np.max(X, axis=0)\n",
    "    X = X / max_array[None, :]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração de matrizes de Features e Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup inicial dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#carrega os dados\n",
    "data_train = load_fashionMNIST_train()\n",
    "data_test = load_fashionMNIST_tests()\n",
    "\n",
    "# Separação em Features e Target\n",
    "\n",
    "data_train_target = np.array(data_train[\"label\"])\n",
    "data_test_target = np.array(data_test[\"label\"])\n",
    "\n",
    "data_train = np.array(data_train)\n",
    "data_test = np.array(data_test)\n",
    "\n",
    "data_train = np.delete(data_train, 0, axis=1)\n",
    "data_test = np.delete(data_test, 0, axis=1)\n",
    "\n",
    "# Normalização\n",
    "normalized_data_train, normalized_data_test = normalize(data_train,data_test)\n",
    "\n",
    "# Scaling\n",
    "scaled_data_train = scale(normalized_data_train)\n",
    "scaled_data_test = scale(normalized_data_test)\n",
    "\n",
    "def getTrainSet():\n",
    "    return scaled_data_train, data_train_target\n",
    "\n",
    "def getTestSet():\n",
    "    return scaled_data_test,data_test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerador de conjuntos de Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Generation \n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# Retorna um gerador de indices\n",
    "def generate_sets(TRAINING_DATA,type='kfold'):\n",
    "\n",
    "    # Cross validation using train_test_split\n",
    "    if (type == 'split'):\n",
    "       return train_test_split(TRAINING_DATA,test_size=0.2,random_state=0)\n",
    "\n",
    "    # Cross validation using K-Fold\n",
    "    # K = 5, Shuffle = true, Seed = 21\n",
    "    elif (type == 'kfold'):\n",
    "        kfold_seed = 21\n",
    "\n",
    "        kfold = KFold(n_splits=5, shuffle=True, random_state=kfold_seed)\n",
    "        return kfold.split(TRAINING_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Units per Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer=784 \n",
    "units_per_hidden_layer=1024 # 2 to the tenth, since there are 10 classes\n",
    "output_layer=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes weights randomly\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "def weights1():\n",
    "    return np.random.rand(input_layer, units_per_hidden_layer)\n",
    "\n",
    "def weights2():\n",
    "    return np.random.rand(units_per_hidden_layer, units_per_hidden_layer)\n",
    "    \n",
    "def weights3():\n",
    "    return np.random.rand(units_per_hidden_layer, output_layer)\n",
    "    \n",
    "     \n",
    "# Adds biases, initialized with zeros\n",
    "\n",
    "def bias1():\n",
    "    return np.zeros((1, units_per_hidden_layer))\n",
    "\n",
    "def bias2():\n",
    "    return np.zeros((1, units_per_hidden_layer))\n",
    "\n",
    "def bias3():\n",
    "    return np.zeros((1, output_layer))\n",
    "\n",
    "def setupNN():\n",
    "    return weights1(), bias1(), weights2(), bias2(),  weights3(), bias3()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid and Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1+np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    return a*(1 - a)\n",
    "\n",
    "def loss(h, y):\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ReLU(z):\n",
    "    z[z<0]=0\n",
    "    return z\n",
    "\n",
    "def ReLU_derivative(z):\n",
    "    z = np.where(z >= 0, 1, 0)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def leaky_ReLU(z):\n",
    "    z = np.where(z > 0, z, z * 0.01)\n",
    "    return z\n",
    "\n",
    "def leaky_ReLU_derivative(z):\n",
    "    z = np.where(z >= 0, 1, 0.01)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    tan = tanh(z)\n",
    "    return (1-np.square(tan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Stable softmax\n",
    "def softmax(z):\n",
    "    expZ = np.exp(z - np.max(z))\n",
    "    return expZ / expZ.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function - Cross Entropy with Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(delta, y, length):\n",
    "    delta[range(length), y] -= 1\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success/Error Percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def error_rate(prediction, y):\n",
    "    return np.mean( prediction != y )\n",
    "\n",
    "def success_rate(prediction, y):\n",
    "    return np.mean( prediction == y )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "#### Different activation functions for hidden layers, but all of them end with the use of Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid_model(completeX, completeY):\n",
    "\n",
    "    W1, B1, W2, B2, W3, B3 = setupNN()\n",
    "\n",
    "    parameters = {}\n",
    "\n",
    "    for k in range(0, epochs):\n",
    "        for j in range(0, int(completeX.shape[0]/batch_size)):\n",
    "            X = completeX[500*j:500*(j+1)]\n",
    "            y = completeY[500*j:500*(j+1)]\n",
    "\n",
    "            # Forward propagation\n",
    "\n",
    "            z1 = np.dot(X, W1) + B1\n",
    "\n",
    "            a1 = sigmoid(z1)\n",
    "\n",
    "            z2 = np.dot(a1, W2) + B2\n",
    "\n",
    "            a2 = sigmoid(z2)\n",
    "\n",
    "            z3 = np.dot(a2, W3) + B3\n",
    "            # Hypothesis\n",
    "            a3 = softmax(z3)\n",
    "\n",
    "            # Back propagation\n",
    "            delta4 = cross_entropy_loss(a3, y, len(X))\n",
    "\n",
    "            dW3 = np.dot(np.transpose(a2), delta4)\n",
    "            dB3 = np.sum(delta4, axis=0, keepdims=True)\n",
    "            dot = np.dot(delta4, np.transpose(W3))\n",
    "            deriv = sigmoid_derivative(a2)\n",
    "            delta3 = np.multiply(dot, deriv)\n",
    "\n",
    "            dW2 = np.dot(np.transpose(a1), delta3)\n",
    "            dB2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "            dot = np.dot(delta3, np.transpose(W2))\n",
    "            deriv = sigmoid_derivative(a1)\n",
    "            delta2 = np.multiply(dot, deriv)\n",
    "\n",
    "            dW1 = np.dot(np.transpose(X), delta2)\n",
    "            dB1 = np.sum(delta2, axis=0, keepdims=True)\n",
    "\n",
    "            # Updating parameters\n",
    "\n",
    "            W1 += -learning_rate * dW1\n",
    "            B1 += -learning_rate * dB1\n",
    "            W2 += -learning_rate * dW2\n",
    "            B2 += -learning_rate * dB2\n",
    "            W3 += -learning_rate * dW3\n",
    "            B3 += -learning_rate * dB3\n",
    "\n",
    "            parameters = {'W1': W1, 'B1': B1, 'W2': W2, 'B2': B2, 'W3': W3, 'B3': B3}\n",
    "\n",
    "#         print(parameters)\n",
    "        \n",
    "        X = completeX[500*j:]\n",
    "        y = completeY[500*j:]\n",
    "\n",
    "        # Forward propagation\n",
    "\n",
    "        z1 = np.dot(X, W1) + B1\n",
    "\n",
    "        a1 = sigmoid(z1)\n",
    "\n",
    "        z2 = np.dot(a1, W2) + B2\n",
    "\n",
    "        a2 = sigmoid(z2)\n",
    "\n",
    "        z3 = np.dot(a2, W3) + B3\n",
    "        # Hypothesis\n",
    "        a3 = softmax(z3)\n",
    "\n",
    "        # Back propagation\n",
    "        delta4 = cross_entropy_loss(a3, y, len(X))\n",
    "\n",
    "        dW3 = np.dot(np.transpose(a2), delta4)\n",
    "        dB3 = np.sum(delta4, axis=0, keepdims=True)\n",
    "        dot = np.dot(delta4, np.transpose(W3))\n",
    "        deriv = sigmoid_derivative(a2)\n",
    "        delta3 = np.multiply(dot, deriv)\n",
    "\n",
    "        dW2 = np.dot(np.transpose(a1), delta3)\n",
    "        dB2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        dot = np.dot(delta3, np.transpose(W2))\n",
    "        deriv = sigmoid_derivative(a1)\n",
    "        delta2 = np.multiply(dot, deriv)\n",
    "\n",
    "        dW1 = np.dot(np.transpose(X), delta2)\n",
    "        dB1 = np.sum(delta2, axis=0, keepdims=True)\n",
    "\n",
    "        # Updating parameters\n",
    "\n",
    "        W1 += -learning_rate * dW1\n",
    "        B1 += -learning_rate * dB1\n",
    "        W2 += -learning_rate * dW2\n",
    "        B2 += -learning_rate * dB2\n",
    "        W3 += -learning_rate * dW3\n",
    "        B3 += -learning_rate * dB3\n",
    "\n",
    "        parameters = {'W1': W1, 'B1': B1, 'W2': W2, 'B2': B2, 'W3': W3, 'B3': B3}\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def sigmoid_prediction(parameters, X):\n",
    "    W1, B1, W2, B2, W3, B3 = parameters['W1'], parameters['B1'], parameters['W2'], parameters['B2'], parameters['W3'], parameters['B3']\n",
    "\n",
    "    z1 = np.dot(X, W1) + B1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + B2\n",
    "    a2 = sigmoid(z2)\n",
    "    z3 = np.dot(a2, W3) + B3\n",
    "    a3 = softmax(z3)\n",
    "\n",
    "    return np.argmax(a3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate for set 0  is:  0.09683333333333333\n",
      "Success rate for set 1  is:  0.09466666666666666\n",
      "Success rate for set 2  is:  0.09483333333333334\n",
      "Success rate for set 3  is:  0.09291666666666666\n",
      "Success rate for set 4  is:  0.10008333333333333\n"
     ]
    }
   ],
   "source": [
    "X1, y1 = getTrainSet()\n",
    "indices_generator = generate_sets(X1)\n",
    "model_list = []\n",
    "result_list = []\n",
    "\n",
    "i=0\n",
    "for train_index, val_index in indices_generator:\n",
    "    x1_train = X1[train_index]\n",
    "    y1_train = y1[train_index]\n",
    "    x1_val = X1[val_index]\n",
    "    y1_val = y1[val_index]\n",
    "     \n",
    "    parameters = sigmoid_model(x1_train, y1_train)\n",
    "    model_list.append(parameters)\n",
    "    \n",
    "    y1_predict = sigmoid_prediction(parameters, x1_val)\n",
    "    success = success_rate(y1_predict, y1_val)\n",
    "    result_list.append(success)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "for i in range(0, len(result_list)):\n",
    "    print('Success rate for set', i, ' is: ', result_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu_model(completeX, completeY):\n",
    "\n",
    "    W1, B1, W2, B2, W3, B3 = setupNN()\n",
    "\n",
    "    parameters = {}\n",
    "    \n",
    "    for k in range(0, epochs):\n",
    "        for j in range(0, int(completeX.shape[0]/batch_size)):\n",
    "            X = completeX[500*j:500*(j+1)]\n",
    "            y = completeY[500*j:500*(j+1)]\n",
    "\n",
    "            # Forward propagation\n",
    "\n",
    "            z1 = np.dot(X, W1) + B1\n",
    "\n",
    "            a1 = ReLU(z1)\n",
    "\n",
    "            z2 = np.dot(a1, W2) + B2\n",
    "\n",
    "            a2 = ReLU(z2)\n",
    "\n",
    "            z3 = np.dot(a2, W3) + B3\n",
    "\n",
    "            # Hypothesis\n",
    "            a3 = softmax(z3)\n",
    "\n",
    "            # Back propagation\n",
    "            delta4 = cross_entropy_loss(a3, y, len(X))\n",
    "\n",
    "            dW3 = np.dot(np.transpose(a2), delta4)\n",
    "            dB3 = np.sum(delta4, axis=0, keepdims=True)\n",
    "            dot = np.dot(delta4, np.transpose(W3))\n",
    "            deriv = ReLU_derivative(a2)\n",
    "            delta3 = np.multiply(dot, deriv)\n",
    "\n",
    "            dW2 = np.dot(np.transpose(a1), delta3)\n",
    "            dB2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "            dot = np.dot(delta3, np.transpose(W2))\n",
    "            deriv = ReLU_derivative(z1)\n",
    "            delta2 = np.multiply(dot, deriv)\n",
    "\n",
    "            dW1 = np.dot(np.transpose(X), delta2)\n",
    "            dB1 = np.sum(delta2, axis=0, keepdims=True)\n",
    "\n",
    "            # Updating parameters\n",
    "\n",
    "            W1 += -learning_rate * dW1\n",
    "            B1 += -learning_rate * dB1\n",
    "            W2 += -learning_rate * dW2\n",
    "            B2 += -learning_rate * dB2\n",
    "            W3 += -learning_rate * dW3\n",
    "            B3 += -learning_rate * dB3\n",
    "\n",
    "            parameters = {'W1': W1, 'B1': B1, 'W2': W2, 'B2': B2, 'W3': W3, 'B3': B3}\n",
    "\n",
    "        X = completeX[500*j:]\n",
    "        y = completeY[500*j:]\n",
    "        \n",
    "        # Forward propagation\n",
    "\n",
    "        z1 = np.dot(X, W1) + B1\n",
    "\n",
    "        a1 = ReLU(z1)\n",
    "\n",
    "        z2 = np.dot(a1, W2) + B2\n",
    "\n",
    "        a2 = ReLU(z2)\n",
    "\n",
    "        z3 = np.dot(a2, W3) + B3\n",
    "\n",
    "        # Hypothesis\n",
    "        a3 = softmax(z3)\n",
    "\n",
    "        # Back propagation\n",
    "        delta4 = cross_entropy_loss(a3, y, len(X))\n",
    "\n",
    "        dW3 = np.dot(np.transpose(a2), delta4)\n",
    "        dB3 = np.sum(delta4, axis=0, keepdims=True)\n",
    "        dot = np.dot(delta4, np.transpose(W3))\n",
    "        deriv = ReLU_derivative(a2)\n",
    "        delta3 = np.multiply(dot, deriv)\n",
    "\n",
    "        dW2 = np.dot(np.transpose(a1), delta3)\n",
    "        dB2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        dot = np.dot(delta3, np.transpose(W2))\n",
    "        deriv = ReLU_derivative(z1)\n",
    "        delta2 = np.multiply(dot, deriv)\n",
    "\n",
    "        dW1 = np.dot(np.transpose(X), delta2)\n",
    "        dB1 = np.sum(delta2, axis=0, keepdims=True)\n",
    "\n",
    "        # Updating parameters\n",
    "\n",
    "        W1 += -learning_rate * dW1\n",
    "        B1 += -learning_rate * dB1\n",
    "        W2 += -learning_rate * dW2\n",
    "        B2 += -learning_rate * dB2\n",
    "        W3 += -learning_rate * dW3\n",
    "        B3 += -learning_rate * dB3\n",
    "\n",
    "        parameters = {'W1': W1, 'B1': B1, 'W2': W2, 'B2': B2, 'W3': W3, 'B3': B3}\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def relu_prediction(parameters, X):\n",
    "    W1, B1, W2, B2, W3, B3 = parameters['W1'], parameters['B1'], parameters['W2'], parameters['B2'], parameters['W3'], parameters['B3']\n",
    "\n",
    "    z1 = np.dot(X, W1) + B1\n",
    "    a1 = ReLU(z1)\n",
    "    z2 = np.dot(a1, W2) + B2\n",
    "    a2 = ReLU(z2)\n",
    "    z3 = np.dot(a2, W3) + B3\n",
    "    a3 = softmax(z3)\n",
    "\n",
    "    return np.argmax(a3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruy/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in subtract\n",
      "  \"\"\"\n",
      "/home/ruy/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in less\n",
      "  after removing the cwd from sys.path.\n",
      "/home/ruy/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate for set 0  is:  0.09683333333333333\n",
      "Success rate for set 1  is:  0.09466666666666666\n",
      "Success rate for set 2  is:  0.09483333333333334\n",
      "Success rate for set 3  is:  0.09291666666666666\n",
      "Success rate for set 4  is:  0.09441666666666666\n"
     ]
    }
   ],
   "source": [
    "X2, y2 = getTrainSet()\n",
    "indices_generator = generate_sets(X2)\n",
    "model_list_relu = []\n",
    "result_list_relu = []\n",
    "\n",
    "i=0\n",
    "for train_index, val_index in indices_generator:\n",
    "    x2_train = X2[train_index]\n",
    "    y2_train = y2[train_index]\n",
    "    x2_val = X2[val_index]\n",
    "    y2_val = y2[val_index]\n",
    "     \n",
    "    parameters = relu_model(x2_train, y2_train)\n",
    "    model_list_relu.append(parameters)\n",
    "    \n",
    "    y2_predict = relu_prediction(parameters, x2_val)\n",
    "    success = success_rate(y2_predict, y2_val)\n",
    "    result_list_relu.append(success)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "for i in range(0, len(result_list)):\n",
    "    print('Success rate for set', i, ' is: ', result_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def leaky_relu_model(completeX, completeY):\n",
    "\n",
    "    W1, B1, W2, B2, W3, B3 = setupNN()\n",
    "\n",
    "    parameters = {}\n",
    "\n",
    "    for k in range(0, epochs):\n",
    "        for j in range(0, int(completeX.shape[0]/batch_size)):\n",
    "            X = completeX[500*j:500*(j+1)]\n",
    "            y = completeY[500*j:500*(j+1)]\n",
    "            \n",
    "            # Forward propagation\n",
    "\n",
    "            z1 = np.dot(X, W1) + B1\n",
    "\n",
    "            a1 = leaky_ReLU(z1)\n",
    "\n",
    "            z2 = np.dot(a1, W2) + B2\n",
    "\n",
    "            a2 = leaky_ReLU(z2)\n",
    "\n",
    "            z3 = np.dot(a2, W3) + B3\n",
    "\n",
    "            # Hypothesis\n",
    "            a3 = softmax(z3)\n",
    "\n",
    "            # Back propagation\n",
    "            delta4 = cross_entropy_loss(a3, y, len(X))\n",
    "\n",
    "            dW3 = np.dot(np.transpose(a2), delta4)\n",
    "            dB3 = np.sum(delta4, axis=0, keepdims=True)\n",
    "            dot = np.dot(delta4, np.transpose(W3))\n",
    "            deriv = leaky_ReLU_derivative(a2)\n",
    "            delta3 = np.multiply(dot, deriv)\n",
    "\n",
    "            dW2 = np.dot(np.transpose(a1), delta3)\n",
    "            dB2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "            dot = np.dot(delta3, np.transpose(W2))\n",
    "            deriv = leaky_ReLU_derivative(z1)\n",
    "            delta2 = np.multiply(dot, deriv)\n",
    "\n",
    "            dW1 = np.dot(np.transpose(X), delta2)\n",
    "            dB1 = np.sum(delta2, axis=0, keepdims=True)\n",
    "\n",
    "            # Updating parameters\n",
    "\n",
    "            W1 += -learning_rate * dW1\n",
    "            B1 += -learning_rate * dB1\n",
    "            W2 += -learning_rate * dW2\n",
    "            B2 += -learning_rate * dB2\n",
    "            W3 += -learning_rate * dW3\n",
    "            B3 += -learning_rate * dB3\n",
    "\n",
    "            parameters = {'W1': W1, 'B1': B1, 'W2': W2, 'B2': B2, 'W3': W3, 'B3': B3}\n",
    "\n",
    "        X = completeX[500*j:]\n",
    "        y = completeY[500*j:]\n",
    "                   \n",
    "        # Forward propagation\n",
    "\n",
    "        z1 = np.dot(X, W1) + B1\n",
    "\n",
    "        a1 = leaky_ReLU(z1)\n",
    "\n",
    "        z2 = np.dot(a1, W2) + B2\n",
    "\n",
    "        a2 = leaky_ReLU(z2)\n",
    "\n",
    "        z3 = np.dot(a2, W3) + B3\n",
    "\n",
    "        # Hypothesis\n",
    "        a3 = softmax(z3)\n",
    "\n",
    "        # Back propagation\n",
    "        delta4 = cross_entropy_loss(a3, y, len(X))\n",
    "\n",
    "        dW3 = np.dot(np.transpose(a2), delta4)\n",
    "        dB3 = np.sum(delta4, axis=0, keepdims=True)\n",
    "        dot = np.dot(delta4, np.transpose(W3))\n",
    "        deriv = leaky_ReLU_derivative(a2)\n",
    "        delta3 = np.multiply(dot, deriv)\n",
    "\n",
    "        dW2 = np.dot(np.transpose(a1), delta3)\n",
    "        dB2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        dot = np.dot(delta3, np.transpose(W2))\n",
    "        deriv = leaky_ReLU_derivative(z1)\n",
    "        delta2 = np.multiply(dot, deriv)\n",
    "\n",
    "        dW1 = np.dot(np.transpose(X), delta2)\n",
    "        dB1 = np.sum(delta2, axis=0, keepdims=True)\n",
    "\n",
    "        # Updating parameters\n",
    "\n",
    "        W1 += -learning_rate * dW1\n",
    "        B1 += -learning_rate * dB1\n",
    "        W2 += -learning_rate * dW2\n",
    "        B2 += -learning_rate * dB2\n",
    "        W3 += -learning_rate * dW3\n",
    "        B3 += -learning_rate * dB3\n",
    "\n",
    "        parameters = {'W1': W1, 'B1': B1, 'W2': W2, 'B2': B2, 'W3': W3, 'B3': B3}\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def leaky_relu_prediction(parameters, X):\n",
    "    W1, B1, W2, B2, W3, B3 = parameters['W1'], parameters['B1'], parameters['W2'], parameters['B2'], parameters['W3'], parameters['B3']\n",
    "\n",
    "    z1 = np.dot(X, W1) + B1\n",
    "    a1 = leaky_ReLU(z1)\n",
    "    z2 = np.dot(a1, W2) + B2\n",
    "    a2 = leaky_ReLU(z2)\n",
    "    z3 = np.dot(a2, W3) + B3\n",
    "    a3 = softmax(z3)\n",
    "\n",
    "    return np.argmax(a3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3, y3 = getTrainSet()\n",
    "indices_generator = generate_sets(X3)\n",
    "model_list_leaky = []\n",
    "result_list_leaky = []\n",
    "\n",
    "i=0\n",
    "for train_index, val_index in indices_generator:\n",
    "    x3_train = X3[train_index]\n",
    "    y3_train = y3[train_index]\n",
    "    x3_val = X3[val_index]\n",
    "    y3_val = y3[val_index]\n",
    "     \n",
    "    parameters = leaky_relu_model(x3_train, y3_train)\n",
    "    model_list_leaky.append(parameters)\n",
    "    \n",
    "    y3_predict = leaky_relu_prediction(parameters, x3_val)\n",
    "    success = success_rate(y3_predict, y3_val)\n",
    "    result_list_leaky.append(success)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "for i in range(0, len(result_list)):\n",
    "    print('Success rate for set', i, ' is: ', result_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tanh_model(completeX, completeY):\n",
    "\n",
    "    W1, B1, W2, B2, W3, B3 = setupNN()\n",
    "\n",
    "    parameters = {}\n",
    "\n",
    "    for k in range(0, epochs):\n",
    "        for j in range(0, int(completeX.shape[0]/batch_size)):\n",
    "            X = completeX[500*j:500*(j+1)]\n",
    "            y = completeY[500*j:500*(j+1)]\n",
    "            \n",
    "            # Forward propagation\n",
    "\n",
    "            z1 = np.dot(X, W1) + B1\n",
    "\n",
    "            a1 = tanh(z1)\n",
    "\n",
    "            z2 = np.dot(a1, W2) + B2\n",
    "\n",
    "            a2 = tanh(z2)\n",
    "\n",
    "            z3 = np.dot(a2, W3) + B3\n",
    "\n",
    "            # Hypothesis\n",
    "            a3 = softmax(z3)\n",
    "\n",
    "            # Back propagation\n",
    "            delta4 = cross_entropy_loss(a3, y, len(X))\n",
    "\n",
    "            dW3 = np.dot(np.transpose(a2), delta4)\n",
    "            dB3 = np.sum(delta4, axis=0, keepdims=True)\n",
    "            dot = np.dot(delta4, np.transpose(W3))\n",
    "            deriv = tanh_derivative(a2)\n",
    "            delta3 = np.multiply(dot, deriv)\n",
    "\n",
    "            dW2 = np.dot(np.transpose(a1), delta3)\n",
    "            dB2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "            dot = np.dot(delta3, np.transpose(W2))\n",
    "            deriv = tanh_derivative(z1)\n",
    "            delta2 = np.multiply(dot, deriv)\n",
    "\n",
    "            dW1 = np.dot(np.transpose(X), delta2)\n",
    "            dB1 = np.sum(delta2, axis=0, keepdims=True)\n",
    "\n",
    "            # Updating parameters\n",
    "\n",
    "            W1 += -learning_rate * dW1\n",
    "            B1 += -learning_rate * dB1\n",
    "            W2 += -learning_rate * dW2\n",
    "            B2 += -learning_rate * dB2\n",
    "            W3 += -learning_rate * dW3\n",
    "            B3 += -learning_rate * dB3\n",
    "\n",
    "            parameters = {'W1': W1, 'B1': B1, 'W2': W2, 'B2': B2, 'W3': W3, 'B3': B3}\n",
    "\n",
    "        X = completeX[500*j:]\n",
    "        y = completeY[500*j:]\n",
    "        \n",
    "        # Forward propagation\n",
    "\n",
    "        z1 = np.dot(X, W1) + B1\n",
    "\n",
    "        a1 = tanh(z1)\n",
    "\n",
    "        z2 = np.dot(a1, W2) + B2\n",
    "\n",
    "        a2 = tanh(z2)\n",
    "\n",
    "        z3 = np.dot(a2, W3) + B3\n",
    "\n",
    "        # Hypothesis\n",
    "        a3 = softmax(z3)\n",
    "\n",
    "        # Back propagation\n",
    "        delta4 = cross_entropy_loss(a3, y, len(X))\n",
    "\n",
    "        dW3 = np.dot(np.transpose(a2), delta4)\n",
    "        dB3 = np.sum(delta4, axis=0, keepdims=True)\n",
    "        dot = np.dot(delta4, np.transpose(W3))\n",
    "        deriv = tanh_derivative(a2)\n",
    "        delta3 = np.multiply(dot, deriv)\n",
    "\n",
    "        dW2 = np.dot(np.transpose(a1), delta3)\n",
    "        dB2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        dot = np.dot(delta3, np.transpose(W2))\n",
    "        deriv = tanh_derivative(z1)\n",
    "        delta2 = np.multiply(dot, deriv)\n",
    "\n",
    "        dW1 = np.dot(np.transpose(X), delta2)\n",
    "        dB1 = np.sum(delta2, axis=0, keepdims=True)\n",
    "\n",
    "        # Updating parameters\n",
    "\n",
    "        W1 += -learning_rate * dW1\n",
    "        B1 += -learning_rate * dB1\n",
    "        W2 += -learning_rate * dW2\n",
    "        B2 += -learning_rate * dB2\n",
    "        W3 += -learning_rate * dW3\n",
    "        B3 += -learning_rate * dB3\n",
    "\n",
    "        parameters = {'W1': W1, 'B1': B1, 'W2': W2, 'B2': B2, 'W3': W3, 'B3': B3}\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def tanh_prediction(parameters, X):\n",
    "    W1, B1, W2, B2, W3, B3 = parameters['W1'], parameters['B1'], parameters['W2'], parameters['B2'], parameters['W3'], parameters['B3']\n",
    "\n",
    "    z1 = np.dot(X, W1) + B1\n",
    "    a1 = tanh(z1)\n",
    "    z2 = np.dot(a1, W2) + B2\n",
    "    a2 = tanh(z2)\n",
    "    z3 = np.dot(a2, W3) + B3\n",
    "    a3 = softmax(z3)\n",
    "\n",
    "    return np.argmax(a3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4, y4 = getTrainSet()\n",
    "indices_generator = generate_sets(X3)\n",
    "model_list_tanh = []\n",
    "result_list_tanh = []\n",
    "\n",
    "i=0\n",
    "for train_index, val_index in indices_generator:\n",
    "    x4_train = X4[train_index]\n",
    "    y4_train = y4[train_index]\n",
    "    x4_val = X4[val_index]\n",
    "    y4_val = y4[val_index]\n",
    "     \n",
    "    parameters = leaky_relu_model(x4_train, y4_train)\n",
    "    model_list_tanh.append(parameters)\n",
    "    \n",
    "    y4_predict = leaky_relu_prediction(parameters, x4_val)\n",
    "    success = success_rate(y4_predict, y4_val)\n",
    "    result_list_tanh.append(success)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "for i in range(0, len(result_list)):\n",
    "    print('Success rate for set', i, ' is: ', result_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xt, yt = getTestSet()\n",
    "\n",
    "# yt_predict = insertmodelhere(insertparameters here, Xt)\n",
    "# success = success_rate(yt_predict, yt)\n",
    "\n",
    "# print('Success rate for the test set is: ', success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.51501377, 0.85790908, 0.85652133, ..., 0.30261102, 0.88268334,\n",
       "         0.0855997 ],\n",
       "        [0.60629368, 0.46126662, 0.09038867, ..., 0.81396873, 0.27073279,\n",
       "         0.98414541],\n",
       "        [0.58693555, 0.65871261, 0.52444706, ..., 0.06179561, 0.22246053,\n",
       "         0.62619336],\n",
       "        ...,\n",
       "        [0.40303253, 0.52293448, 0.75669702, ..., 0.53001388, 0.10609088,\n",
       "         0.51219279],\n",
       "        [0.05273732, 0.45906282, 0.23880027, ..., 0.74135558, 0.42224536,\n",
       "         0.6631642 ],\n",
       "        [0.37532945, 0.43790288, 0.72499746, ..., 0.81551447, 0.93672773,\n",
       "         0.44091107]]),\n",
       " 'B1': array([[0.00443691, 0.00174879, 0.02495292, ..., 0.03824668, 0.00671888,\n",
       "         0.00275444]]),\n",
       " 'W2': array([[0.5138675 , 0.17369268, 0.50396098, ..., 0.64768592, 0.06103932,\n",
       "         0.57780742],\n",
       "        [0.45862141, 0.9112465 , 0.90495222, ..., 0.13810448, 0.00159307,\n",
       "         0.00248142],\n",
       "        [0.70422367, 0.81887256, 0.37361108, ..., 0.46177892, 0.90457415,\n",
       "         0.28631965],\n",
       "        ...,\n",
       "        [0.16237288, 0.92356253, 0.80181126, ..., 0.26177294, 0.6456399 ,\n",
       "         0.10170853],\n",
       "        [0.570293  , 0.79656265, 0.12626534, ..., 0.66335011, 0.56018146,\n",
       "         0.66681787],\n",
       "        [0.22282154, 0.408008  , 0.84503621, ..., 0.24020389, 0.52996972,\n",
       "         0.18171012]]),\n",
       " 'B2': array([[2.79778715, 3.28245468, 2.79113757, ..., 3.04338278, 3.00741464,\n",
       "         2.76152794]]),\n",
       " 'W3': array([[48.2381057 , 49.16870564, 48.49968669, ..., 48.62826867,\n",
       "         49.26618689, 48.73318859],\n",
       "        [49.05358981, 49.12568871, 48.64329022, ..., 48.50059504,\n",
       "         48.82203424, 49.18710302],\n",
       "        [48.27159358, 49.18094203, 48.47491825, ..., 48.67346998,\n",
       "         49.0698855 , 48.22777849],\n",
       "        ...,\n",
       "        [48.06853301, 49.09440023, 48.76470003, ..., 48.53478576,\n",
       "         48.69047572, 49.14391872],\n",
       "        [48.79098282, 49.10284829, 49.21754102, ..., 48.32813205,\n",
       "         49.2228787 , 49.15758741],\n",
       "        [48.21744488, 48.46440433, 48.39720084, ..., 48.23184623,\n",
       "         49.31537906, 48.25573403]]),\n",
       " 'B3': array([[48.31931647, 48.59980083, 48.51188444, 47.93999997, 48.24994766,\n",
       "         48.99355313, 47.83555979, 48.69999966, 48.53993816, 48.3699999 ]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
